%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Some auxiliary results on convex functions}
\label{Sec:AuxiliaryResults}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

In this appendix we present some auxiliary results that are needed in the paper. Such results
concern convex functions. Recall that for measurable functions $f:\R\to \R$, convexity in an open
interval $I$ is equivalent to midpoint convexity, i.e.,
$$
\dfrac{f(x) + f(y)}{2} \geq f \left( \dfrac{x+y}{2}\right) \quad \textrm{ for every } x,\, y \in I\,,
$$
and the same is true for strict convexity with an strict inequality
(see Chapter~1 of \cite{Niculescu} and the references therein).


\begin{lemma}
\label{Lemma:Convex<->AllReflectionsConvex} Let $h: I \subseteq \R \to \R$ be a function defined in
an open interval $I$ such that it is measurable. Then, $h(z)$ is convex in $I$ if and only if
$\widetilde{h}_c(z) := h(c+z) + h(c-z)$ is convex in $I_c := (-\dist\{c, \partial I\}, \dist\{c,
\partial I\})$ for every $c\in I$. The statement remains true if we replace convexity by strict
convexity, concavity or strict concavity.
\end{lemma}

\begin{proof}
Assume first that $h$ is convex in $I$. We call
$$
x_+ = c + x\,, \quad x_- = c - x\,, \quad y_+ = c + y\,, \quad \textrm{ and } \quad y_- = c - y\,.
$$
Therefore, if $x$, $y\in I_c$, we have that $x_+$, $x_-$, $y_+$, $y_- \in I$. Hence, for all $t\in
(0,1)$
\begin{align*}
t\widetilde{h}_c(x) + (1-t)\widetilde{h}_c(y)
&=  th(x_+) + (1-t)h(y_+) + t h(x_-) + (1-t)h(y_-) \\
&\geq h(tx_+ + (1-t)y_+) + h(tx_- + (1-t)y_-) \\
&= h(c + tx + (1-t)y) + h(c-tx + (1-t)y) \\
& = \widetilde{h}_c(tx + (1-t)y)\,.
\end{align*}
Therefore, $\widetilde{h}_c(z)$ is convex in $I_c$ for every $c\in I$.

Assume now that $\widetilde{h}_c(z)$ is convex in $I_c$ for every $c\in I$. By contradiction,
suppose that $h$ is not convex in $I$. Then, there exist some $x$, $y\in I$ such that
\begin{equation}
\label{Eq:ContradictionConvexity}
\dfrac{h(x) + h(y)}{2} < h \left (\dfrac{x+y}{2}\right )\,.
\end{equation}

Let $c = (x+y)/2$ and thus
$$
\widetilde{h}_c(z) = h\left( \dfrac{x+y}{2} + z\right) +  h\left( \dfrac{x+y}{2} - z\right)\,.
$$
Define $ x_0 := (x-y)/2$ and $y_0:= (y-x)/2$. It is clear that $x_0$, $y_0\in I_c$. Therefore,
$$
h(x) + h(y) = \dfrac{1}{2} \left( \widetilde{h}_c(x_0) + \widetilde{h}_c(y_0)\right )
\geq \widetilde{h}_c \left( \dfrac{x_0 + y_0}{2}\right )
= 2 h \left (\dfrac{x+y}{2}\right )\,,
$$
and this contradicts \eqref{Eq:ContradictionConvexity}. Hence, $h$ is convex in $I$.
\end{proof}


\begin{corollary}
\label{Cor:gConvex<->K(sqrt)convex} Let $K:(0,+\infty) \to (0,+\infty)$ be a measurable function.
Then, given any $c_1,c_2>0$, the function
$$
g(z) := K \left (c_1 \sqrt{1 + c_2 z}\right) +  K \left (c_1 \sqrt{1 - c_2 z}\right)
$$
is  (strictly) convex in $(-1/c_2, 1/c_2)$ for every $c_1>0$ if and only if $K(\sqrt{z})$ is
(strictly) convex in $(0, +\infty)$.
\end{corollary}
\begin{proof}
Since we can rewrite $g$ as
$$
g(z) = K \left (\sqrt{c_1^2 + c_1^2c_2 z}\right) +  K \left (\sqrt{c_1^2 - c_1^2c_2 z}\right),
$$
it is clear that $g$ is  (strictly) convex in $(-1/c_2, 1/c_2)$ for every $c_1>0$ if and only if
$$
K \left(\sqrt{c_1^2 + z}\right) +  K \left(\sqrt{c_1^2 - z}\right)
$$
is (strictly) convex in $(-c_1^2, c_1^2)$ for every $c_1>0$. Then, by applying
Lemma~\ref{Lemma:Convex<->AllReflectionsConvex} taking $I = (0,+\infty)$, this is equivalent to the
convexity of $K(\sqrt{z})$ in $(0, +\infty)$.
\end{proof}

\begin{lemma}
\label{Lemma:InequalitConvexFunctions} Let $g \colon I\subset \R \to \R$ be a function defined in
an open interval $I$ such that $g$ is measurable and nondecrasing in $I$. Then, we have the
following equivalences:
\begin{enumerate}
\item[i)] $g$ is strictly convex in $I$.
\item[ii)] For any given real numbers $A$, $B$, $C$, $D$ $\in I$ such that
\begin{equation}
\label{Eq:AssumptionsInequalitiesABCD}
\begin{cases}
A = \max\{A,B,C,D\}\,, \\
A + D \geq B + C\,,
\end{cases}
\end{equation}
it is satisfied that
$$
g(A) + g(D) \geq g(B) + g(C)
$$
and equality holds if and only if
$$ A = B \ \ \ \ \textrm{ and} \ \ \ \ C=D, $$
or
$$ A = C \ \ \ \ \textrm{ and} \ \ \ \ B=D. $$
\end{enumerate}

\end{lemma}
\begin{proof}
$i)\, \Rightarrow \,ii)$ First note that from the strict convexity condition in $g$ we have that
$g$ is $C^0(I)$ and $C^1$ at all but at most countably many points. Moreover, $g'$ is increasing in
the regular points. In addition, from being $g$ strictly convex and nondecreasing, it is in fact
increasing.

Without loss of generality, we can assume that $B \geq C$. Now, we distinguish two cases:

$\bullet D \geq C$

This is the simpler case. Since $g$ is nondecreasing and $A \geq B$, $D \geq C$, we have that $g(A)
\geq g(B)$ and $g(D) \geq g(C)$. By adding up these two inequalities we get the desired result.
Moreover, if $A>B$ or $D>C$ we obtain the strict inequality since $g$ is in fact increasing.

$\bullet D < C$

If $A = B$ we arrive at a contradiction from \eqref{Eq:AssumptionsInequalitiesABCD}. Then, we have
$A > B \geq C > D$.

Since $g$ is piecewise $C^1$, let $\{z_i\}_{i=1}^{n}$ be the points in the interval $(B,A)$ where
the function is not differentiable, with $z_0 = B$ and $z_{n+1}=A$, and let
$\{\bar{z}_i\}_{i=1}^{m}$ be the same for the interval $(D,C)$ with $\bar{z}_0 = D$ and
$\bar{z}_{m+1}=C$.

By applying Taylor's theorem in each interval of differentiability and adding the expressions we
obtain
$$
g(A) = g(B) + \sum_{i=0}^{n}(z_{i+1}-z_i) g'(\xi_i) \quad \textrm{ with } \xi_i \in (z_i,z_{i+1})\,,
$$
and
$$
g(C) = g(D) + \sum_{i=0}^{m}(\bar{z}_{i+1}-\bar{z}_i) g'(\bar{\xi}_i) \quad \textrm{ with } \bar{\xi}_i \in (\bar{z}_i,\bar{z}_{i+1})\,,
$$
If we define
$$ \xi = \xi_0 \ \ \ \ \textrm{ and } \ \ \ \ \bar{\xi} = \bar{\xi}_m
,$$
since $g'$ is nondecreasing in the regular points we get
$$
g(A) \geq g(B) + \sum_{i=0}^{n}(z_{i+1}-z_i) g'(\xi) = g(B) + (A-B)g'(\xi),
$$
and
$$
g(C) \leq g(D) + \sum_{i=0}^{m}(\bar{z}_{i+1}-\bar{z}_i) g'(\bar{\xi}) = g(D)+(C-D)g'(\bar{\xi}).
$$
It is clear that $\xi > \overline{\xi}$ and then $g'(\xi) > g'(\overline{\xi})$. Therefore,
\begin{align*}
g(A) + g(D) - g(B) - g(C) &= (g(A) - g(B)) - (g(C) - g(D)) \\
&\geq (A - B)g'(\xi)  - (C - D)g'(\overline{\xi}) \\
&= (A - B - C + D)g'(\xi) + (C - D) (g'(\xi) - g'(\overline{\xi}))\\
& > 0\,,
\end{align*}
where we have used that $A - B - C + D \geq 0$, $C - D > 0$, $g'\geq 0$ in the regular points and
$g'(\xi) > g'(\overline{\xi})$.

$ii)\, \Rightarrow \,i)$ Given $x\neq y$ in $I$, that we can suppose $x>y$ without loss of
generality, we define $A=x$, $B=C=(x+y)/2$ and $D=y$. Then we get
$$ g(x)+g(y) > 2g\left( \frac{x+y}{2} \right), $$
that from being $g$ measurable means it is strictly convex.
\end{proof}

\begin{remark}
\label{Remark:InequalitConvexFunctions} Note that the condition of strict convexity is only needed
in order to characterize when the equality is satisfied. That is, with only a convexity condition
we also obtain the inequality although we are not able to determine when equality is satisfied.
\end{remark}
\begin{remark}
\label{Remark:LeftImplicationDoNotRequireNondecreasing}
The deduction of $i)$ from $ii)$ does not require $g$ to be nondecreasing.
\end{remark}

\begin{corollary}
\label{Cor:hDecreasingConvex} Let $h \colon I\subset \R \to \R$ be a function defined in an open
interval $I$ such that $h$ is measurable and nonincreasing in $I$. Then, we have the following
equivalences:
\begin{enumerate}
\item[i)] $h$ is convex in $I$.
\item[ii)] For any given real numbers $a$, $b$, $c$, $d$ $\in I$ such that
\begin{equation}
\label{Eq:AssumptionsInequalitiesabcd}
\begin{cases}
a \geq b \geq c \geq d \,, \\
a + d \leq b + c\,.
\end{cases}
\end{equation}
it is satisfied that
$$ h(a) + h(d) \geq h(b) + h(c)\,.$$
\end{enumerate}
\end{corollary}

\begin{proof}
Let us define $g(z) = h(a-z)$. It is clear that since $h$ is measurable and nonincreasing, then $g$
is measurable and nondecreasing. On the other hand, let $A=a-d$, $B=a-c$, $C=a-b$ and $D=0$. Then,
we have that condition $a \geq b \geq c \geq d$ is equivalent to $A\geq B \geq C \geq D$ and
condition $a+d\leq b+c$ is equivalent to $A+D\geq B+C$. Therefore, we can apply Lemma
\ref{Lemma:InequalitConvexFunctions}, taking into account Remark
\ref{Remark:InequalitConvexFunctions}, and the desired equivalence is obtained.
\end{proof}


\begin{lemma}
\label{Lemma:gNondecreasing}
Let $K:(0,+\infty) \to (0,+\infty)$ be a measurable function such that
$$ \lim_{z\to+\infty} K(z) = 0 $$
and $ K(\sqrt{z}) $ is convex/strictly convex in $(0,+\infty)$. Then, given any $c_1,c_2>0$ the
function
$$
g(z) := K(c_1 \sqrt{1 + c_2 z}) +  K(c_1 \sqrt{1 - c_2 z})
$$
is nondecreasing/increasing in $(0, 1/c_2)$.
\end{lemma}

\begin{proof}
First, note that from the hypothesis of $K$ we can deduce that $K(\sqrt{z})$, and also
$K(c_1\sqrt{z})$, is convex and nonincreasing.

Now, given $x\geq y$, we have
\begin{align*}
g(x)-g(y) &= K(c_1\sqrt{1+c_2 x}) + K(c_1\sqrt{1-c_2 x}) \\
&\ \ \ \ - K(c_1\sqrt{1+c_2 y}) -K(c_1\sqrt{1-c_2 y}) \geq 0,
\end{align*}
where we have applied Corollary~\ref{Cor:hDecreasingConvex} with $h(z) = K(c_1\sqrt{z})$,
$a=1+c_2x$, $b=1+c_2y$, $c=1-c_2y$ and $d=1-c_2x$.
\end{proof}

\begin{remark}
If we assume that $K$ is nonincreasing and concave, then we can prove in a similar way that $g$ is
nondecreasing.
\end{remark}


\begin{proposition}
\label{Prop:EquivalenceK(sqrt)Convex<->Inequality}
Let $K:(0, +\infty) \to (0,+\infty)$ be a continuous and nonincreasing function such that
$$
\lim_{z\to +\infty} K(z) = 0\,.
$$
Then, the following statements are equivalent:
\begin{enumerate}
\item[i)] $K(\sqrt{z})$ is strictly convex in $(0, +\infty)$.
\item[ii)] For every $c_1$, $c_2$, $A$, $B$, $C$ and $D$ satisfying
  \begin{enumerate}
    \item $c_1 > 0$, $c_2>0$.
    \item $A$, $B$, $C$, $D \in (0, 1/c_2)$.
	\item $A = \max\{A,\, B,\, C,\, D\}$.
    \item $A + D \geq B + C$.
  \end{enumerate}
  it holds
  $$
  g(A) + g(D) \geq g(B) + g(C)\,,
  $$
  where
  $$
  g(z) := K(c_1 \sqrt{1 + c_2z}) + K(c_1 \sqrt{1 - c_2z})\,.
  $$
  Moreover, equality holds if and only if
  $$ A = B \ \ \ \ \textrm{ and} \ \ \ \ C=D, $$
  or
  $$ A = C \ \ \ \ \textrm{ and} \ \ \ \ B=D. $$
\end{enumerate}
\end{proposition}

\begin{proof}
$i)\, \Rightarrow \,ii)$ By Lemma~\ref{Lemma:gNondecreasing} and
Corollary~\ref{Cor:gConvex<->K(sqrt)convex}, $g$ is nondecreasing and strictly convex in
$(-1/c_2,1/c_2)$ for all $c_1>0$. Therefore, point $ii)$ follows from
Lemma~\ref{Lemma:InequalitConvexFunctions}.

$ii)\, \Rightarrow \,i)$ By Lemma~\ref{Lemma:InequalitConvexFunctions} and in view of
Remark~\ref{Remark:LeftImplicationDoNotRequireNondecreasing}, $g$ is strictly convex in
$(-1/c_2,1/c_2)$ for all $c_1>0$. Hence, using Lemma~\ref{Cor:gConvex<->K(sqrt)convex} we deduce
that $K(\sqrt{z})$ is strictly convex in $(0, +\infty)$.
\end{proof}


\bigskip
-------------------
\bigskip


\begin{lemma}
\label{Lemma:ComputationABCD} Let $\alpha$, $\beta$ two real numbers satisfying $\alpha \geq
|\beta|$. Let $x=(x',x'')$, $y=(y',y'')\in \ocal \subset \R^{2m}$. Define
$$
\begin{array}{cc}
	A = |x'||y'|  \alpha + |x''||y''|\beta \,, &
	B = |x'||y''| \alpha + |x''||y'| \beta \,, \\
	C = |x''||y'| \alpha + |x'||y''| \beta \,, &
	D = |x''||y''|\alpha + |x'||y'|  \beta \,.
\end{array}
$$
Then,
\begin{enumerate}
\item It holds
$$
\begin{cases}
|A| \geq |B|,\ |A| \geq|C|, \ |A| \geq|D|\,, \\
|A| + |D| \geq |B| + |C|\,.
\end{cases}
$$
\item If either
$$ |A| = |B| \ \ \ \ \textrm{ and} \ \ \ \ |C| = |D|, $$
or
$$ |A| = |C| \ \ \ \ \textrm{ and} \ \ \ \ |B| = |D|, $$
then necessarily $\alpha = \beta = 0$.
\end{enumerate}

\end{lemma}
\begin{proof}
Since $\alpha \geq |\beta |$,
$$
\alpha\geq 0 \quad \textrm{ and } \quad  -\alpha \leq \beta \leq \alpha\,.
$$
Moreover, since $x,y\in\ocal$ it holds
$$
|x'|>|x''| \quad \textrm{ and } \quad |y'|>|y''|\,.
$$
These inequalities will be used in all the proof.

We start with the first point. First,  we show that $A\geq 0$ and that
$$
A \geq |B|, \ A \geq |C| ,\ A \geq |D|\,.
$$


- $A \geq 0$:
$$
 A =  |x'||y'|  \alpha + |x''||y''|\beta \geq (|x'||y'|  - |x''||y''|)\alpha \geq 0\,.
$$

- $A \geq |B|$:
$$
A\pm B = (|x'|\alpha-|x''|\beta)(|y'|\pm |y''|) \geq 0\,.
$$

- $A \geq |C|$:
$$
A\pm C = (|y'|\alpha-|y''|\beta)(|x'|\pm |x''|)  \geq 0\,.
$$

- $A \geq |D|$:
$$
A\pm D = (|x'||y'| \pm |x''||y''|)(\alpha \pm \beta) \geq 0\,.
$$


It remains to show
$$
A + |D| \geq |B| + |C|\,.
$$
The proof of it is just a computation considering all the eight possible configurations of the
signs of $B$, $C$ and $D$. Since the roles of $B$ and $C$ are completely interchangeable, we may
assume that $B \geq C$ and we only need to check six cases. To do it, note first that
\begin{equation}
\label{Eq:LemmaABCDProof1}
A + D - B - C = (|x'|-|x''|)(|y'|-|y''|)(\alpha + \beta) \geq 0 \,,
\end{equation}
\begin{equation}
\label{Eq:LemmaABCDProof2}
A - D - B + C = (|x'|+|x''|)(|y'|-|y''|)(\alpha - \beta) \geq 0 \,,
\end{equation}
and
\begin{equation}
\label{Eq:LemmaABCDProof3}
A + D + B + C = (|x'|+|x''|)(|y'|+|y''|)(\alpha + \beta) \geq 0 \,,
\end{equation}
With these three relations at hand we check the six cases.

- If $B \geq 0$, $C \geq 0$ and $D \geq 0$, then by \eqref{Eq:LemmaABCDProof1} we have
$$
A + |D| - |B| - |C| = A + D - B - C \geq 0\,.
$$

- If $B \geq 0$, $C \geq 0$ and $D \leq 0$, we use that $D\leq 0$ and \eqref{Eq:LemmaABCDProof1} to
see that
$$
A + |D| - |B| - |C| = A - D - B - C =  A + D - B - C -2D \geq 0\,.
$$

- If $B \geq 0$, $C \leq 0$ and $D \geq 0$, we use that $D\geq 0$ and \eqref{Eq:LemmaABCDProof2} to
see that
$$
A + |D| - |B| - |C| = A + D - B + C =  A - D - B + C +2D \geq 0\,.
$$

- If $B \geq 0$, $C \leq 0$ and $D \leq 0$, then by \eqref{Eq:LemmaABCDProof2} we have
$$
A + |D| - |B| - |C| = A - D - B + C \geq 0\,.
$$

- If $B \leq 0$, $C \leq 0$ and $D \geq 0$, then by \eqref{Eq:LemmaABCDProof3} we have
$$
A + |D| - |B| - |C| = A + D + B + C \geq 0\,.
$$

- If $B \leq 0$, $C \leq 0$ and $D \leq 0$, we use that $D\leq 0$ and \eqref{Eq:LemmaABCDProof3} to
see that
$$
A + |D| - |B| - |C| = A - D + B + C =  A + D + B + C -2D \geq 0\,.
$$

This concludes the proof of the first statement.

We prove now the second point of the lemma. Since the roles of $B$ and $C$ are completely
interchangeable, we only need to show the result in the case $|A| = |B|$ and $|C| = |D|$.

Recall that $A \geq 0$. Hence, since $A = |B|$ and $|C| = |D|$, a simple computation shows that
$$
\alpha = \sign (B) \dfrac{|x''|}{|x'|}\beta \quad \textrm{ and } \quad
\beta = \sign (C) \sign(D) \dfrac{|x''|}{|x'|} \alpha \,.
$$
Hence,
$$
\alpha = \sign (B) \sign (C) \sign(D) \dfrac{|x''|^2}{|x'|^2} \alpha
$$
and if we assume $\alpha \neq 0$, then necessarily $\sign (B) \sign (C) \sign(D)=1$ and $|x'| =
|x''|$, but this is a contradiction with $x\in \ocal$. Therefore, $\alpha = 0$ and thus $\beta =
0$.
\end{proof}








%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\section{Computations in $s$ and $t$ coordinates}
\label{Sec:stcomputations}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{The nonlocal Allen-Cahn equation in the $(s,t)$ variables}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

The goal of this subsection is to write equation \eqref{Eq:NonlocalAllenCahn} in the $(s,t)$
variables. For this, we must find a new kernel in these variables.

\begin{lemma}
\label{Lemma:OperatorInSTVariables} Let $m \geq 1$, $\s\in(0,1)$ and let $u\in
C_{\loc}^\alpha(\R^{2m})$ with $\alpha > 2\s$ be a function depending only on the variables $s$ and
$t$. Let $L$ be an operator of the form \eqref{Eq:DefOfLu} with a kernel $K\geq 0$ satisfying $K(y)
= K(|y|)$. Then, for any $x = (s x_s, t x_t)$ with $x_s$, $x_t$ $\in \Sph^{m-1}$ ($x_s$, $x_t = \pm
1$ in the case $m=1$),  $Lu(x)$ can be written in the following way:\todo{Is singular in $s=0$ or
$t=0$??}
\begin{equation}
\label{Eq:OperatorInSTVariables}
Lu(x) = \widetilde{L} u (s,t) := \int_0^{+\infty}  \int_0^{+\infty} \sigma^{m-1} \tau^{m-1} \big(u(s,t) - u(\sigma, \tau)\big) J(s,t,\sigma, \tau)  \d \sigma\d \tau\,,
\end{equation}
where:
\begin{enumerate}
	\item If $m= 1$,
	\begin{equation}
		\label{Eq:KernelInSTVariablesR2}
	J(s,t,\sigma, \tau) := \sum_{i=0}^1  \sum_{j =0}^1  K\Big(\sqrt{s^2 + t^2 + \sigma^2 + \tau^2 -2 s \sigma (-1)^i -2 t \tau (-1)^j}\Big)\,.
	\end{equation}
	
	\item If $m\geq 2$,
	\begin{align}
	J(s,t,\sigma, \tau) &:= \int_{\Sph^{m-1}}  \int_{\Sph^{m-1}}  K\Big(\sqrt{s^2 + t^2 + \sigma^2 + \tau^2 -2 s \sigma \omega_1 -2 t \tau \tilde{\omega}_1}\Big) \d \omega \d \tilde{\omega}  \label{Eq:KernelSTVariables1}\\
	&= c_m \int_{-1}^1  \int_{-1}^1  (1-\theta^2)^{\frac{m-3}{2}} (1-\overline{\theta}^2)^{\frac{m-3}{2}} \nonumber\\
	& \quad \quad \quad \quad \quad
	K\Big(\sqrt{s^2 + t^2 + \sigma^2 + \tau^2 -2 s \sigma \theta -2 t \tau \overline{\theta}}\Big) \d \theta \d \overline{\theta}\,, \label{Eq:KernelSTVariables2}
	\end{align}
	with
	$$
	c_m = \bpar{\dfrac{2 \pi^{\frac{m-1}{2}}}{\Gamma (\frac{m-1}{2})}}^2.
	$$
\end{enumerate}
%To compute it one should split three cases:
%$
%c_m = \begin{cases}  \dfrac{1}{\pi^2} & m= 2 \,,\\
%1 &  m= 3\,,\\
%\ds 4\pi^2 \prod_{k=1}^{m-3} \bpar{\int_0^\pi \sin^k \theta \d \theta }^2 & m \geq 4\,.
%\end{cases}
%$}
\end{lemma}


\begin{proof}
We start with the case $m=1$. In this case, we will use explicitly that since $u$ is a function of
$s$ and $t$, then $u$ is even with respect to the coordinate axis. Using this symmetry and the
change $y = -\tilde{y}$, we have
$$
Lu(x) = \int_{\{y_2 > - y_1\}} \big( u(x) - u(y)\big) \{K(|x - y|) + K(|x + y|)\} \d y\,.
$$
If we call
$$
I(\Omega, x) := \int_{\Omega} \big( u(x) - u(y)\big) \{K(|x - y|) + K(|x + y|)\} \d y\,,
$$
then
$$
Lu(x) = I(\{y_2 > |y_1|\},x) + I(\{y_1 > |y_2|\},x)\,.
$$
We will check that $I(\{y_2 > |y_1|\},x)$ can be written in the form
\eqref{Eq:OperatorInSTVariables} (integrated in the set $\tau > \sigma$). The computations for
$I(\{y_1 > |y_2|\},x)$ are completely analogous.

First, note that the set $\{y_2 > |y_1|\}$ can be written as
$$
\{y_2 > y_1 > 0\} \cup \phi(\{y_2 > y_1 > 0\}) \cup \{y_2 > 0, y_1 =0\}
$$
where $\phi$ is the reflection with respect to the $y_2$-axis. Therefore,
\begin{align*}
I(\{y_2 > |y_1|\}, x) & = \int_{\{y_2 > y_1 > 0\}} \big( u(x) - u(y)\big) \{K(|x - y|) + K(|x + y|)\} \d y  \\
& \quad \quad + \int_{\phi(\{y_2 > y_1 > 0\})} \big( u(x) - u(y)\big) \{K(|x - y|) + K(|x + y|)\} \d y\,.
\end{align*}
By performing the change $\phi = \phi^{-1}$ in the second integral and using the symmetry of $u$,
we end up with
\begin{align*}
I(\{y_2 > |y_1|\}, x) &=
\int_{\{y_2 > y_1 > 0\}} \big( u(x) - u(y)\big) \cdot\\
& \quad \quad \left \{K(|x - y|) + K(|x + y|) + K(|x - \phi_1(y)|) + K(|x + \phi_1(y)|) \right \} \d y\,.
\end{align*}
Then, if in the previous expression we write
$$
x = (s \sign(x_1), t \sign(x_2)) \quad \textrm{ and } \quad = (\sigma \sign(y_1), \tau \sign(y_2))\,,
$$
we find that
$$
I(\{y_2 > |y_1|\}, x) =
\int_0^{\infty} \! \! \d \sigma \int_\sigma^{\infty} \! \! \d \tau \ \   \big( u(s,t) - u(\sigma, \tau)\big) J(s,t,\sigma, \tau)\,,
$$
with $J$ as in \eqref{Eq:KernelInSTVariablesR2}. Indeed, in $\{y_2 > y_1 > 0\}$ we have that
$y=(\sigma, \tau)$, and it is not difficult to check that the expression
$$
K(|x - y|) + K(|x + y|) + K(|x - \phi_1(y)|) + K(|x + \phi_1(y)|)
$$
does not depend on $\sign(x_1)$ nor $\sign(x_1)$, so we can assume that $x=(s,t)$ and then
\begin{align*}
K(|x - y|) + K(|x + y|) + K(|x - \phi_1(y)|) + K(|x + \phi_1(y)|) = \\
K\Big(\sqrt{s^2 + t^2 + \sigma^2 + \tau^2 + 2 s \sigma + 2 t \tau }\Big) +  K\Big(\sqrt{s^2 + t^2 + \sigma^2 + \tau^2 + 2 s \sigma  -2 t \tau }\Big) \\
\quad + K\Big(\sqrt{s^2 + t^2 + \sigma^2 + \tau^2 -2 s \sigma + 2 t \tau }\Big) + K\Big(\sqrt{s^2 + t^2 + \sigma^2 + \tau^2 -2 s \sigma  -2 t \tau }\Big)\,.
\end{align*}

In a completely analogous way, we find that
$$
I(\{y_1 > |y_2|\},x) = \int_0^{\infty} \! \! \d \sigma \int_0^\sigma \! \! \d \tau \ \   \big( u(s,t) - u(\sigma, \tau)\big) J(s,t,\sigma, \tau)\,,
$$
and hence
$$
Lu(x) = \int_0^{\infty} \! \! \d \sigma \int_0^\infty \! \! \d \tau \ \   \big( u(s,t) - u(\sigma, \tau)\big) J(s,t,\sigma, \tau) =: \widetilde{L}u(s,t)\,.
$$
This concludes the proof when $m=1$.
	
We deal now with the case $m=2$. Let $x = (s x_s, t x_t)$ with $x_s$, $x_t$ $\in \Sph^{m-1}$ and $y = (\sigma y_\sigma, \tau y_\tau)$ with $y_\sigma$, $y_\tau$ $\in \Sph^{m-1}$.
Then,
\begin{align*}
Lu(x) &= \int_{\R^{2m}} \big( u(x) - u(y)\big) K( |x-y|) \d y &\\
&= \int_0^{+\infty}  \int_0^{+\infty} \sigma^{m-1} \tau^{m-1} \big(u(s,t) - u(\sigma, \tau)\big)  \\
&\quad \quad \quad \quad  \bpar{\int_{\Sph^{m-1}}  \int_{\Sph^{m-1}} K \Big( \sqrt{|sx_s - \sigma y_\sigma|^2 + |t x_t - \tau y_\tau|^2 } \Big) \d y_\sigma \d y_\tau } \d \sigma \d \tau
\end{align*}
Now, we make some manipulations to the term
\begin{equation}
\label{Eq:KernelSTVariablesProof1}
J(x_s, x_t, s,t,\sigma, \tau) := \int_{\Sph^{m-1}}  \int_{\Sph^{m-1}} K \Big( \sqrt{|sx_s - \sigma y_\sigma|^2 + |t x_t - \tau y_\tau|^2 }\Big ) \d y_\sigma \d y_\tau \,.
\end{equation}

First of all, it is easy to see that \eqref{Eq:KernelSTVariablesProof1} does not depend on $x_s$
nor $x_t$. Indeed, consider a different point $(z_s, z_t)$ and let $M_s$ and $M_t$ be two
orthogonal transformations such that $M_s(x_s) = z_s$ and $M_t(x_t) = z_t$. Then, making the change
of variables $y_\sigma = M_s(\tilde{y}_\sigma)$ and $y_\tau = M_t(\tilde{y}_\tau)$, and using that
$M_s( \Sph^{m-1}) = M_t(\Sph^{m-1}) = \Sph^{m-1}$, we find out that
\begin{align*}
& \hspace{-1cm} J(z_s, z_t, s,t,\sigma, \tau) = \\
&= \int_{\Sph^{m-1}}  \int_{\Sph^{m-1}} K \Big( \sqrt{|s M_s(x_s) - \sigma y_\sigma|^2 + |t M_t(x_t) - \tau y_\tau|^2 }\Big) \d y_\sigma \d y_\tau \\
&= \int_{\Sph^{m-1}}  \int_{\Sph^{m-1}} K \Big( \sqrt{|s M_s(x_s) - \sigma M_s(\tilde{y}_\sigma)|^2 + |t M_t(x_t) - \tau M_t(\tilde{y}_\tau)|^2 }\Big) \d \tilde{y}_\sigma \d \tilde{y}_\tau \\
&= \int_{\Sph^{m-1}}  \int_{\Sph^{m-1}} K\Big ( \sqrt{|M_s(sx_s - \sigma \tilde{y}_\sigma)|^2 + |M_t(t x_t - \tau \tilde{y}_\tau)|^2 }\Big) \d \tilde{y}_\sigma \d \tilde{y}_\tau \\
&= \int_{\Sph^{m-1}}  \int_{\Sph^{m-1}} K\Big ( \sqrt{|sx_s - \sigma \tilde{y}_\sigma|^2 + |t x_t - \tau \tilde{y}_\tau|^2 }\Big) \d \tilde{y}_\sigma \d \tilde{y}_\tau \\
&= J(x_s, x_t, s,t,\sigma, \tau) \,.
\end{align*}

Therefore, we can replace $x_s$ and $x_t$ in \eqref{Eq:KernelSTVariablesProof1} by $e =(1,0,\ldots,
0) \in \Sph^{m-1}$. Thus, we have
\begin{equation}
\label{Eq:KernelSTVariablesProof2}
J(s,t,\sigma, \tau) := \int_{\Sph^{m-1}}  \int_{\Sph^{m-1}} K\Big( \sqrt{|s e - \sigma y_\sigma|^2 + |t e - \tau y_\tau|^2 }\Big) \d y_\sigma \d y_\tau \,.
\end{equation}
For an easier notation, we rename $\omega = y_\sigma$ and $\tilde\omega = y_\tau$, and thus we have
\begin{align*}
|s e - \sigma y_\sigma|^2 + |t e - \tau y_\tau|^2 &= |s e - \sigma \omega|^2 + |t e - \tau \tilde\omega|^2\\
&= s^2 +\sigma^2 - 2 s \sigma e \cdot \omega + t^2 + \tau^2 - 2 t \tau e\cdot \tilde\omega \\
&= s^2 +\sigma^2 - 2 s \sigma \omega_1 + t^2 + \tau^2 - 2t \tau\tilde\omega_1\,.
\end{align*}
This gives \eqref{Eq:KernelSTVariables1}. Finally, we use polar coordinates in $\Sph^{m-1}$ to
obtain \eqref{Eq:KernelSTVariables2}.
\end{proof}







Now we rewrite the kernel inequality \eqref{Eq:KernelInequalityReflexion} in $s,t$-coordinates.
\todo[inline]{Poner solo la ineq o tambien la demo?}

\begin{lemma}
\label{Lemma:KernelInequalityCone} 
Let $m\geq 2$ and let $J$ the kernel defined in
\eqref{Eq:KernelSTVariables2}. Then, if $s>t>0$ and $\sigma > \tau>0$\todo{Check if we can put
$t=0$ or $\tau = 0$}, we have \todo{Kernel strictly decreasing???}
\begin{equation}
\label{Eq:KernelInequalityCone}
J(s,t,\sigma, \tau) \geq J(s,t,\tau, \sigma)\,.
\end{equation}
\end{lemma}

\begin{proof}
We will just consider some simplifications of \eqref{Eq:KernelInequalityCone}. Eventually, we will
use Lemma~\ref{Lemma:InequalitConvexFunctions} to deduce the desired inequality.

First of all, note that it is enough to show that
\begin{equation}
\label{Eq:KernelInequalitySimplified1}
\int_{-1}^1  \int_{-1}^1  (1-\alpha^2)^{\frac{m-3}{2}} (1-\beta^2)^{\frac{m-3}{2}}  \left \{\tilde{K}\Big(\sqrt{1 -2 s \sigma \alpha -2 t \tau \beta}\Big) - \tilde{K}\Big(\sqrt{1 -2 s \tau \alpha -2 t \sigma \beta}\Big)  \right \}\d \alpha \d \beta \geq 0\,,
\end{equation}
Where $\tilde{K}(z) = K((s^2 + t^2 + \sigma^2 + \tau^2)z)$. To see that this is equivalent to
\eqref{Eq:KernelInequalityCone}, we just normalize the variables in the following way:
$$
\tilde{s} = \dfrac{s}{\sqrt{s^2 + t^2 + \sigma^2 + \tau^2}}\,, \quad \tilde{t} = \dfrac{t}{\sqrt{s^2 + t^2 + \sigma^2 + \tau^2}}\,,
$$
$$
\tilde{\sigma} = \dfrac{\sigma}{\sqrt{s^2 + t^2 + \sigma^2 + \tau^2}}\, \quad \textrm{ and } \quad \tilde{\tau} = \dfrac{\tau}{\sqrt{s^2 + t^2 + \sigma^2 + \tau^2}}\,.
$$

The second simplification is the following. Since $s>t>0$ and $\sigma > \tau>0$, we may write
$$
s = (1+\varepsilon) t \quad \textrm{ and} \quad \sigma = (1 + \delta) \tau
$$
with $\varepsilon$, $\delta > 0$. Then, \eqref{Eq:KernelInequalitySimplified1} is equivalent to
\begin{equation}
\label{Eq:KernelInequalitySimplified2}
\int_{-1}^1  \int_{-1}^1  (1-\alpha^2)^{\frac{m-3}{2}} (1-\beta^2)^{\frac{m-3}{2}}  \left \{\tilde{K}\Big(\sqrt{1 -2 t \tau \{(1+\varepsilon)(1+\delta) \alpha + \beta\}}\Big) - \tilde{K}\Big(\sqrt{1 -2 t \tau \{(1+\varepsilon) \alpha + (1+\delta)\beta\}}\Big)  \right \}\d \alpha \d \beta \geq 0\,.
\end{equation}

Now, we make some changes of variables to reduce the domain of integration. First, we divide
$(-1,1)^2 \setminus \{|\alpha| = |\beta|\}$ into four sectors:
$$
Q_1 = \{\alpha > |\beta| \}\,, \quad Q_2 = \{\beta > |\alpha| \}\,, \quad Q_3 = \{\alpha < -|\beta|\}\,, \quad \textrm{ and } \quad Q_4 = \{ \beta < -|\alpha|\}\,.
$$
Consider the changes
\begin{align*}
  \psi_2 \colon Q_2 \  &\to \ Q_1 \\
  \ (\alpha,\beta) &\mapsto (\beta,\alpha)
\end{align*}
\begin{align*}
  \psi_3 \colon Q_3 \  &\to \ Q_1 \\
  \ (\alpha,\beta) &\mapsto (-\alpha,-\beta)
\end{align*}
\begin{align*}
  \psi_4 \colon Q_4 \  &\to \ Q_1 \\
  \ (\alpha,\beta) &\mapsto (-\beta,-\alpha)
\end{align*}
Then, \eqref{Eq:KernelInequalitySimplified2} is now equivalent to show
\begin{align*}
\int \int_{Q_1} \left\{
  \tilde{K}\Big(\sqrt{1 -2 t \tau \{(1+\varepsilon)(1+\delta) \alpha + \beta\}}\Big)
+ \tilde{K}\Big(\sqrt{1 +2 t \tau \{(1+\varepsilon)(1+\delta) \alpha + \beta\}}\Big) \right. \\
+ \tilde{K}\Big(\sqrt{1 -2 t \tau \{(1+\varepsilon)(1+\delta) \beta + \alpha\}}\Big)
+ \tilde{K}\Big(\sqrt{1 +2 t \tau \{(1+\varepsilon)(1+\delta) \beta + \alpha\}}\Big) \\
- \tilde{K}\Big(\sqrt{1 -2 t \tau \{(1+\varepsilon) \alpha + (1+\delta)\beta\}}\Big)
- \tilde{K}\Big(\sqrt{1 +2 t \tau \{(1+\varepsilon) \alpha + (1+\delta)\beta\}}\Big) \\
\left.- \tilde{K}\Big(\sqrt{1 -2 t \tau \{(1+\varepsilon) \beta + (1+\delta)\alpha\}}\Big)
- \tilde{K}\Big(\sqrt{1 +2 t \tau \{(1+\varepsilon) \beta + (1+\delta)\alpha\}}\Big)
\right\} \cdot \\
\cdot (1-\alpha^2)^{\frac{m-3}{2}} (1-\beta^2)^{\frac{m-3}{2}} \d \alpha \d \beta \geq 0\,.
\end{align*}
Note that we are not considering anymore the set $\{|\alpha| = |\beta|\}$, that has measure zero.

Now, if we call
$$
g(z) := \tilde{K}\Big(\sqrt{1 -2 t \tau z}\Big)
+ \tilde{K}\Big(\sqrt{1 +2 t \tau z }\Big)\,,
$$
the previous inequality reads
\begin{equation}
\label{Eq:KernelInequalitySimplified3}
\begin{split}
	\int \int_{Q_1} \left\{
	g\Big((1+\varepsilon)(1+\delta) \alpha + \beta\Big)
	+ g\Big((1+\varepsilon)(1+\delta) \beta + \alpha\Big)
	\right.\\
	\left.
	- g\Big((1+\varepsilon) \alpha + (1+\delta)\beta\Big)
	- g\Big((1+\varepsilon) \beta + (1+\delta)\alpha\Big)
	\right\} \cdot \\
	\cdot (1-\alpha^2)^{\frac{m-3}{2}} (1-\beta^2)^{\frac{m-3}{2}} \d \alpha \d \beta \geq 0\,.
\end{split}
\end{equation}
We claim that
\begin{equation}
\label{Eq:KernelInequalityLastSimplification}
g\Big((1+\varepsilon)(1+\delta) \alpha + \beta\Big)
+ g\Big((1+\varepsilon)(1+\delta) \beta + \alpha\Big)
 \geq
g\Big((1+\varepsilon) \alpha + (1+\delta)\beta\Big)
+g\Big((1+\varepsilon) \beta + (1+\delta)\alpha\Big) \,.
\end{equation}
This will conclude the proof.

To prove the claim, we want to use Lemma~\ref{Lemma:InequalitConvexFunctions} with
$$
\begin{array}{cc}
A = (1+\varepsilon)(1+\delta) \alpha + \beta\,, &
B = (1+\varepsilon) \alpha + (1+\delta)\beta\,, \\
C = (1+\delta)\alpha + (1+\varepsilon) \beta\,, &
D = \alpha + (1+\varepsilon)(1+\delta) \beta\,.
\end{array}
$$
Note that, since $\tilde{K}$ and $\sqrt{1 - z}$ are nonincreasing, $g$ is nondecreasing $[0,1)$.
Moreover, since $\tilde{K}$ is convex and nonincreasing and $\sqrt{1 - z}$ is concave, $g$ is
convex in $[0,1)$. But since $A$, $B$, $C$ and $D$ $\in (-1, 1)$ ---by the normalizations we have
made--- and $g$ is even, we cannot apply directly Lemma~\ref{Lemma:InequalitConvexFunctions} ($g$
is nonincreasing in $(-1,0]$). Instead, if we do it for  $|A|$, $|B|$, $|C|$ and $|D|$, then we can
use the lemma in $[0,1)$. Hence, we should check that
$$
\begin{cases}
|A| \geq |B|,\ |C|, \ |D|\,, \\
|A| + |D| \geq |B| + |C|\,.
\end{cases}
$$
The verification of these inequalities is a simple but tedious computation and it is presented in
the appendix (see Lemma~\ref{Lemma:ComputationABCD}). Once we have these inequalities, we use
Lemma~\ref{Lemma:InequalitConvexFunctions} to deduce
$$
g(|A|) + g(|D|) \geq g(|B|) + g(|C|)\,,
$$
which is equivalent to \eqref{Eq:KernelInequalityLastSimplification} since $g$ is even. This
concludes the proof of \eqref{Eq:KernelInequalityCone}.



Finally, to justify that the inequality in \eqref{Eq:KernelInequalityCone} is strict up to a set of
measure zero, we consider the points where $J(s,t,\sigma, \tau) = J(s,t,\tau,\sigma)$. Following
the previous arguments, this is equivalent to say that we have an equality in
\eqref{Eq:KernelInequalitySimplified2}, and since the integrand of that equality is nonnegative, it
is equivalent to say that we have an equality in \eqref{Eq:KernelInequalityLastSimplification} up
to a set of measure zero. Then, we take into account the following: since the function $\sqrt{1-z}$
is increasing and strictly convex and the kernel $K$ is decreasing, then $g''>0$ in $(0,1)$.
Therefore, in view of Remark~\eqref{Remark:StrictInequalitConvexFunctions}, the equality $g(A) +
g(D) = g(B) + g(C)$ is only possible in the case $A=B=C=D$, and it is easy to verify that this
cannot happen unless $A=B=C=D=0$\todo{Check again}. This is equivalent to $s=t$ and $\sigma=\tau$,
something that is impossible in $\ocal$.

%, under these conditions, $\varepsilon = \delta = (\sqrt{5} - 1)/2$. If we fix such values of $\varepsilon$ and $\delta$ we have a set of measure zero, $ \{2 \tilde{s} = (1 + \sqrt{5}) \tilde{t} \} \cap \{2 \tilde{\sigma} = (1 + \sqrt{5}) \tilde{\tau} \}$, in the space of the normalized parameters $\tilde{s}$, $\tilde{t}$, $\tilde{\sigma}$, $\tilde{\tau}$, as well as in the space of original ones.
\end{proof}

